{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d585e6",
   "metadata": {},
   "source": [
    "## SNP MACE & Severity Prediction Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec70c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, log_loss\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.optimize import minimize\n",
    "from xgboost import  XGBClassifier\n",
    "import statsmodels.api as sm\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eee61b",
   "metadata": {},
   "source": [
    "#### 1. Feature Selection: Lasso Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_snp_selection(data_path, target_col, n_bootstrap=100):\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Genetic features only\n",
    "    X_cols = [c for c in df.columns if c.startswith('SNP') or c == 'Variant.Pathogene']\n",
    "    X = df[X_cols]\n",
    "    y = df[target_col].astype(int) \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X_cols)\n",
    "    \n",
    "    # C grid for stability selection\n",
    "    C_grid = [0.1, 0.2, 0.5, 1.0]\n",
    "    \n",
    "    all_selections = np.zeros(len(X_cols))\n",
    "    \n",
    "    print(f\"Running Stability Selection for {target_col} ({n_bootstrap} bootstraps)...\")\n",
    "    \n",
    "    for b in tqdm(range(n_bootstrap)):\n",
    "        # Resample with stratification to handle any class imbalance\n",
    "        X_res, y_res = resample(X_scaled, y, stratify=y, random_state=b)\n",
    "        \n",
    "        bootstrap_selection = np.zeros(len(X_cols), dtype=bool)\n",
    "        for C_val in C_grid:\n",
    "    \n",
    "            clf = LogisticRegression(penalty='l1', solver='liblinear', C=C_val, \n",
    "                                     random_state=b, multi_class='ovr')\n",
    "            clf.fit(X_res, y_res)\n",
    "            \n",
    "            # Non-zero in any class\n",
    "            selected_mask = np.any(np.abs(clf.coef_) > 1e-4, axis=0)\n",
    "            bootstrap_selection |= selected_mask\n",
    "            \n",
    "        all_selections += bootstrap_selection\n",
    "                    \n",
    "    stability_scores = all_selections / n_bootstrap\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'SNP': X_cols,\n",
    "        'Stability_Score': stability_scores\n",
    "    }).sort_values(by='Stability_Score', ascending=False)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6426c",
   "metadata": {},
   "source": [
    "#### 2. Genetic Feature Engineering: ADR Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_snps(df, target_col, snp_prefix='SNP'):\n",
    "    \"\"\"\n",
    "    Categorizes SNPs into Additive, Dominant, or Recessive models \n",
    "    based on AIC (Akaike Information Criterion) from Logistic Regression.\n",
    "    \n",
    "    Mapping rules:\n",
    "    - Additive: 0, 1, 2 (No change)\n",
    "    - Dominant: 0 -> 0, (1, 2) -> 1\n",
    "    - Recessive: (0, 1) -> 0, 2 -> 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify SNP columns\n",
    "    snp_cols = [col for col in df.columns if col.startswith(snp_prefix)]\n",
    "    \n",
    "    y = df[target_col]\n",
    "    adr_results = {}\n",
    "    \n",
    "    # print(f\"Categorizing {len(snp_cols)} SNPs against {target_col}...\")\n",
    "    \n",
    "    for snp in tqdm(snp_cols):\n",
    "        X_orig = df[snp]\n",
    "        \n",
    "        # 1. Additive Model (0, 1, 2)\n",
    "        try:\n",
    "            model_add = sm.Logit(y, sm.add_constant(X_orig)).fit(disp=0)\n",
    "            aic_add = model_add.aic\n",
    "        except:\n",
    "            aic_add = np.inf\n",
    "            \n",
    "        # 2. Dominant Model (0 vs 1,2)\n",
    "        X_dom = (X_orig >= 1).astype(int)\n",
    "        if X_dom.nunique() > 1:\n",
    "            try:\n",
    "                model_dom = sm.Logit(y, sm.add_constant(X_dom)).fit(disp=0)\n",
    "                aic_dom = model_dom.aic\n",
    "            except:\n",
    "                aic_dom = np.inf\n",
    "            \n",
    "        # 3. Recessive Model (0,1 vs 2)\n",
    "        X_rec = (X_orig == 2).astype(int)\n",
    "        if X_rec.nunique() > 1:\n",
    "            try:\n",
    "                model_rec = sm.Logit(y, sm.add_constant(X_rec)).fit(disp=0)\n",
    "                aic_rec = model_rec.aic\n",
    "            except:\n",
    "                aic_rec = np.inf\n",
    "            \n",
    "        # Determine best model\n",
    "        aics = {'Additive': aic_add, 'Dominant': aic_dom, 'Recessive': aic_rec}\n",
    "        best_model = min(aics, key=aics.get)\n",
    "        \n",
    "        # If all models failed or are inf, default to Additive\n",
    "        if aics[best_model] == np.inf:\n",
    "            best_model = 'Additive'\n",
    "            \n",
    "        adr_results[snp] = best_model\n",
    "        \n",
    "    return adr_results\n",
    "\n",
    "def apply_adr_encoding(df, adr_mapping):\n",
    "    \"\"\"\n",
    "    Applies the ADR mapping to the dataframe.\n",
    "    \"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    for snp, model in adr_mapping.items():\n",
    "        if snp not in df_encoded.columns:\n",
    "            continue\n",
    "            \n",
    "        if model == 'Dominant':\n",
    "            df_encoded[snp] = (df_encoded[snp] >= 1).astype(int)\n",
    "        elif model == 'Recessive':\n",
    "            df_encoded[snp] = (df_encoded[snp] == 2).astype(int)\n",
    "        # Additive remains as is (0, 1, 2)\n",
    "        \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4c639",
   "metadata": {},
   "source": [
    "#### 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc7b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few new features created.\n",
    "def engineer_clinical_features(df):\n",
    "    \"\"\"\n",
    "    Extract structural and functional interactions from clinical data.\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    # 1. Atrial Loading: young patients with large atrial diameter\n",
    "    new_df['Atrial_Loading_Index'] = new_df['Diam_OG'] * new_df['Age_Baseline']\n",
    "    # 2. Obstruction-Symptom Risk: Gradient interacting with syncope\n",
    "    new_df['Obstruction_Symptom_Risk'] = new_df['Gradient'] * new_df['SYNCOPE']\n",
    "    # 3. Hypertrophy to Size Ratio: Normalized wall thickness\n",
    "    new_df['Hypertrophy_BSA_Ratio'] = new_df['Epaiss_max'] / (new_df['BSA'] + 1e-6)\n",
    "    # 4. Pump Efficiency Index: FEVG vs Atrial Size\n",
    "    new_df['Pump_Efficiency_Index'] = new_df['FEVG'] / (new_df['Diam_OG'] + 1e-6)\n",
    "    # 5. Wall Stress Proxy (Pressure x Thickness)\n",
    "    new_df['Wall_Stress_Proxy'] = new_df['Gradient'] * new_df['Epaiss_max']\n",
    "    # 6. Non-linear Age Risk\n",
    "    new_df['Age_Baseline_Sq'] = new_df['Age_Baseline'] ** 2\n",
    "\n",
    "    # 7. Clinical Risk Thresholding (Categorization) - User Requested\n",
    "    new_df['Massive_Hypertrophy'] = (new_df['Epaiss_max'] >= 30).astype(int)\n",
    "    new_df['Obstruction_Level'] = (new_df['Gradient'] >= 30).astype(int)\n",
    "    new_df['Heart_Failure_Warning'] = (new_df['FEVG'] < 50).astype(int)\n",
    "\n",
    "    # 8. Additional Logical Features\n",
    "    new_df['Disease_Duration'] = np.maximum(0, new_df['Age_Baseline'] - new_df['Age_Diag'])\n",
    "    new_df['BMI_High_Risk'] = (new_df['BMI'] >= 30).astype(int)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708a404",
   "metadata": {},
   "source": [
    "#### 4. Final Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7efca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridConsensusClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    A custom hybrid ensemble classifier that combines a Bootstrapped Linear model\n",
    "    with a set of non-linear classifiers.\n",
    "    \"\"\"\n",
    "    def __init__(self, stable_snps, n_iterations=250, weightage=None, pct_sample=0.7, random_state=42):\n",
    "        self.stable_snps = stable_snps\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weightage = weightage\n",
    "        self.random_state = random_state\n",
    "        self.pct_sample = pct_sample\n",
    "        \n",
    "        # Initialize base non-linear models\n",
    "        self.non_linear_models = {\n",
    "            'forest': RandomForestClassifier(n_estimators=10, random_state=random_state),\n",
    "            'svm': SVC(kernel='linear', C=0.5, probability=True, random_state=random_state),\n",
    "            'knn': KNeighborsClassifier(n_neighbors=len(stable_snps) if stable_snps else 10),\n",
    "            'bayes': GaussianNB(),\n",
    "            'xgb': XGBClassifier(n_estimators=10, random_state=random_state, eval_metric='logloss')\n",
    "        }\n",
    "        \n",
    "    def _fit_linear_stability(self, X, y):\n",
    "        \"\"\"\n",
    "        Implementation of bootstrapped beta coefficient estimation.\n",
    "        \"\"\"\n",
    "        classes = np.sort(np.unique(y))\n",
    "        num_classes = len(classes)\n",
    "        \n",
    "        # Determine sampling size based on minority class for balancing\n",
    "        class_counts = pd.Series(y).value_counts()\n",
    "        min_class_size = class_counts.min()\n",
    "        sample_size = int(min_class_size * self.pct_sample)\n",
    "        \n",
    "        all_coefs = []\n",
    "        all_intercepts = []\n",
    "        \n",
    "        print(f\"Bootstrapping Linear Model parameters ({self.n_iterations} iterations)...\")\n",
    "        for i in tqdm(range(self.n_iterations)):\n",
    "            # Create balanced subsample\n",
    "            indices = []\n",
    "            for cls in classes:\n",
    "                cls_indices = np.where(y == cls)[0]\n",
    "                picked = np.random.choice(cls_indices, sample_size, replace=False)\n",
    "                indices.extend(picked)\n",
    "            \n",
    "            X_sub = X.iloc[indices][self.stable_snps]\n",
    "            y_sub = y[indices]\n",
    "            \n",
    "            if num_classes > 2:\n",
    "                model = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "            else:\n",
    "                model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "            \n",
    "            model.fit(X_sub, y_sub)\n",
    "            all_coefs.append(model.coef_)\n",
    "            all_intercepts.append(model.intercept_)\n",
    "            \n",
    "        self.lin_coefs_ = np.mean(all_coefs, axis=0)\n",
    "        self.lin_intercept_ = np.mean(all_intercepts, axis=0)\n",
    "        self.classes_ = classes\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits all models in the ensemble.\n",
    "        X: DataFrame or Array-like\n",
    "        y: Array-like\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=self.stable_snps if hasattr(self, 'stable_snps') else None)\n",
    "        \n",
    "        y_array = np.array(y)\n",
    "        \n",
    "        # 1. Fit Linear Model via Bootstrapping\n",
    "        self._fit_linear_stability(X, y_array)\n",
    "        \n",
    "        # 2. Fit Non-Linear Models\n",
    "        print(\"Fitting non-linear ensemble models...\")\n",
    "        X_selected = X[self.stable_snps]\n",
    "        for name, model in self.non_linear_models.items():\n",
    "            model.fit(X_selected, y_array)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict_all_probas(self, X):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of probabilistic predictions from each internal model.\n",
    "        \"\"\"\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=self.stable_snps)\n",
    "            \n",
    "        X_selected = X[self.stable_snps]\n",
    "        n_classes = len(self.classes_)\n",
    "        \n",
    "        all_probs = {}\n",
    "        \n",
    "        # 1. Linear Prediction (Sigmoid/Softmax using averaged weights)\n",
    "        z = np.dot(X_selected, self.lin_coefs_.T) + self.lin_intercept_\n",
    "        if n_classes == 2:\n",
    "            # Binary case\n",
    "            prob_1 = 1 / (1 + np.exp(-z)).flatten()\n",
    "            lin_probs = np.vstack([1 - prob_1, prob_1]).T\n",
    "        else:\n",
    "            # Multiclass case (Softmax)\n",
    "            exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "            lin_probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        \n",
    "        all_probs['linear'] = lin_probs\n",
    "        \n",
    "        # 2. Collect Non-Linear Probabilities\n",
    "        for name, model in self.non_linear_models.items():\n",
    "            all_probs[name] = model.predict_proba(X_selected)\n",
    "            \n",
    "        return all_probs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predicts weighted probabilities.\n",
    "        \"\"\"\n",
    "        all_probs = self.predict_all_probas(X)\n",
    "        \n",
    "        # 3. Apply Weighting\n",
    "        if self.weightage is None:\n",
    "            # Default: Equal weightage\n",
    "            weight_val = 1.0 / len(all_probs)\n",
    "            self.final_weights_ = {m: weight_val for m in all_probs.keys()}\n",
    "        else:\n",
    "            # Normalized user weights\n",
    "            total = sum(self.weightage.values())\n",
    "            self.final_weights_ = {m: v / total for m, v in self.weightage.items()}\n",
    "            # Any model not in weightage gets 0\n",
    "            for m in all_probs.keys():\n",
    "                if m not in self.final_weights_:\n",
    "                    self.final_weights_[m] = 0.0\n",
    "\n",
    "        # Aggregate weighted probabilities\n",
    "        weighted_probs = np.zeros_like(all_probs['linear'])\n",
    "        for name, p in all_probs.items():\n",
    "            weighted_probs += self.final_weights_[name] * p\n",
    "            \n",
    "        return weighted_probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class with the highest weighted probability.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(probs, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc449816",
   "metadata": {},
   "source": [
    "*Load data the fill any nan values present*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a748026",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"cardihack_final-train.csv\"\n",
    "test_data_path = \"cardihack_final-test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "CLINICAL_COLS = ['Age_Baseline', 'Age_Diag', 'BMI', 'BSA', 'Genre', 'Epaiss_max', 'Gradient', 'TVNS', 'FEVG', 'ATCD_MS', 'SYNCOPE', 'Diam_OG']\n",
    "\n",
    "for col in CLINICAL_COLS:\n",
    "    fill_val = train_df[col].mean()\n",
    "    train_df[col] = train_df[col].fillna(fill_val)\n",
    "    test_df[col] = test_df[col].fillna(fill_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8c141",
   "metadata": {},
   "source": [
    "*SNP selection and model prediction for `OUTCOME SEVERITY` and `OUTCOME MACE`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select Stable SNPs for OUTCOME SEVERITY\n",
    "severity_stability = stable_snp_selection(train_data_path, 'OUTCOME SEVERITY')\n",
    "STABLE_SNPS_SEVERITY =  severity_stability[severity_stability['Stability_Score']>=0.9]['SNP'].tolist()\n",
    "\n",
    "# 2. Apply ADR Encoding\n",
    "snp_maps_sev = categorize_snps(train_df[STABLE_SNPS_SEVERITY+['OUTCOME SEVERITY']], 'OUTCOME SEVERITY')\n",
    "mapped_train_data_sev = apply_adr_encoding(train_df, snp_maps_sev)\n",
    "mapped_test_data_sev = apply_adr_encoding(test_df, snp_maps_sev)\n",
    "\n",
    "# 5. Train SNP Hybrid Ensemble Component with the optimized weight.\n",
    "weights_sev = {\n",
    "    'linear': 0.4049, \n",
    "    'knn': 0.3404, \n",
    "    'bayes': 0.1789, \n",
    "    'xgb': 0.0758\n",
    "}\n",
    "model_sev = HybridConsensusClassifier(\n",
    "    stable_snps=STABLE_SNPS_SEVERITY, \n",
    "    n_iterations=250, \n",
    "    weightage=weights_sev\n",
    ")\n",
    "model_sev.fit(mapped_train_data_sev[STABLE_SNPS_SEVERITY], mapped_train_data_sev['OUTCOME SEVERITY'])\n",
    "outcome_severity_prediction = model_sev.predict_proba(mapped_test_data_sev[STABLE_SNPS_SEVERITY])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e8cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select Stable SNPs for OUTCOME MACE\n",
    "mace_stability = stable_snp_selection(train_data_path, 'OUTCOME MACE')\n",
    "STABLE_SNPS_MACE = mace_stability[mace_stability['Stability_Score']>=0.9]['SNP'].tolist()\n",
    "\n",
    "# 2. Apply ADR Encoding\n",
    "snp_maps_mace = categorize_snps(train_df[STABLE_SNPS_MACE+['OUTCOME MACE']], 'OUTCOME MACE')\n",
    "mapped_train_mace = apply_adr_encoding(train_df, snp_maps_mace)\n",
    "mapped_test_mace = apply_adr_encoding(test_df, snp_maps_mace)\n",
    "\n",
    "# 3. Clinical Feature Engineering (Interactions)\n",
    "train_v4 = engineer_clinical_features(train_df)\n",
    "test_v4 = engineer_clinical_features(test_df)\n",
    "FEAT_V4 = CLINICAL_COLS + [\n",
    "    'Atrial_Loading_Index', 'Obstruction_Symptom_Risk', \n",
    "    'Hypertrophy_BSA_Ratio', 'Pump_Efficiency_Index', \n",
    "    'Wall_Stress_Proxy', 'Age_Baseline_Sq', 'Massive_Hypertrophy', 'Obstruction_Level','Heart_Failure_Warning',\n",
    "    'Disease_Duration','BMI_High_Risk'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_clm = scaler.fit_transform(train_v4[FEAT_V4])\n",
    "X_test_clm = scaler.transform(test_v4[FEAT_V4])\n",
    "\n",
    "# 4. Train Clinical Specialist High-Performance Models\n",
    "print(\"Training Super Clinical Specialists v4 (Logistic, RF, MLP)...\")\n",
    "clm_log = LogisticRegression(multi_class='multinomial', max_iter=1000).fit(X_train_clm, train_df['OUTCOME MACE'])\n",
    "clm_rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42).fit(X_train_clm, train_df['OUTCOME MACE'])\n",
    "clm_mlp = MLPClassifier(hidden_layer_sizes=(32,16), max_iter=1000, random_state=42).fit(X_train_clm, train_df['OUTCOME MACE'])\n",
    "\n",
    "# 5. Train SNP Hybrid Ensemble Component\n",
    "print(\"Training SNP Hybrid Ensemble Component...\")\n",
    "snp_hybrid = HybridConsensusClassifier(stable_snps=STABLE_SNPS_MACE, n_iterations=250)\n",
    "snp_hybrid.fit(mapped_train_mace, mapped_train_mace['OUTCOME MACE'])\n",
    "snp_probs_dict = snp_hybrid.predict_all_probas(mapped_test_mace)\n",
    "\n",
    "# 6. Global Weighted Blending (Optimized)\n",
    "GLOBAL_WEIGHTS_V4 = {\n",
    "    'Clm_Logistic': 0.0134, \n",
    "    'Clm_RF': 0.1595, \n",
    "    'Clm_MLP': 0.1032,\n",
    "    'linear': 0.0943, \n",
    "    'forest': 0.2495, \n",
    "    'svm': 0.1074, \n",
    "    'knn': 0.0111, \n",
    "    'bayes': 0.2586, \n",
    "    'xgb': 0.0031\n",
    "}\n",
    "\n",
    "print(\"Applying Optimized Global Weights v4...\")\n",
    "clm_probs = {\n",
    "    'Clm_Logistic': clm_log.predict_proba(X_test_clm), \n",
    "    'Clm_RF': clm_rf.predict_proba(X_test_clm), \n",
    "    'Clm_MLP': clm_mlp.predict_proba(X_test_clm)\n",
    "}\n",
    "\n",
    "mace_probs = np.zeros_like(clm_probs['Clm_Logistic'])\n",
    "for name, w in GLOBAL_WEIGHTS_V4.items():\n",
    "    p = clm_probs[name] if name.startswith('Clm') else snp_probs_dict[name]\n",
    "    mace_probs += w * p\n",
    "\n",
    "outcome_mace_prediction = np.argmax(mace_probs, axis=1)\n",
    "print(\"MACE Prediction Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d412d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final prediction and submit.\n",
    "\n",
    "test_df['OUTCOME SEVERITY'] = outcome_severity_prediction\n",
    "test_df['OUTCOME MACE'] = outcome_mace_prediction\n",
    "\n",
    "test_df.to_csv(\"Submission.csv\", index=False)\n",
    "print(\"Final predictions with Advanced Clinical Features saved to Submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
